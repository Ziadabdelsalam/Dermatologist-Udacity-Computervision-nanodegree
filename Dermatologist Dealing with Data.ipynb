{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np \n",
    "from glob import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_files size: 2000\n",
      "train_files shape: (2000,)\n",
      "target shape: (2000, 3)\n",
      "['melanoma', 'nevus', 'seborrheic_keratosis']\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_path, shuffle=None):\n",
    "    kwargs = {}\n",
    "    if shuffle != None:\n",
    "        kwargs['shuffle'] = shuffle\n",
    "    data = load_files(data_path, **kwargs)\n",
    "    img_files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), 3)\n",
    "    return img_files, targets\n",
    "\n",
    "train_files, train_targets = load_dataset('data/train')\n",
    "valid_files, valid_targets = load_dataset('data/valid')\n",
    "test_files, test_targets = load_dataset('data/test', shuffle=False)\n",
    "\n",
    "# load lables\n",
    "label_name = [item[11:-1] for item in sorted(glob(\"data/train/*/\"))]\n",
    "\n",
    "print('train_files size: {}'.format(len(train_files)))\n",
    "print('train_files shape: {}'.format(train_files.shape))\n",
    "print('target shape: {}'.format(train_targets.shape))\n",
    "print(label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from tqdm import tqdm \n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path,target_size=(384,256))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "def paths_to_tensor(image_paths):\n",
    "    return np.vstack([path_to_tensor(path)for path in image_paths])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:19<00:00,  5.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:35<00:00,  4.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [03:24<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 384, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "#loading images into tensors \n",
    "train_tensors = paths_to_tensor(tqdm(train_files))\n",
    "\n",
    "valid_tensors = paths_to_tensor(tqdm(valid_files))\n",
    "\n",
    "test_tensors = paths_to_tensor(tqdm(test_files))\n",
    "\n",
    "\n",
    "print(train_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "apply_train_image_tranform = False \n",
    "if apply_train_image_tranform:\n",
    "    datagen_train = ImageDataGenerator(horizontal_flip=True\n",
    "                                      ,vertical_flip=True)\n",
    "    datagen_train.fit(train_tensors)\n",
    "    shape = (train_tensors.shape[0]*2,)+train_tensors.shape[1:]\n",
    "    generated = np.ndarray(shape=shape)\n",
    "    for i , image in tqdm(enumerate(train_tensors)):\n",
    "        generated[i]=datagen_train.random_transform(image)\n",
    "        train_tensors=np.concatenate((train_tensors,generated))\n",
    "        train_targets=train_targets.repeat(2, axis=0)\n",
    "train_imgs_preprocess = preprocess_input(train_tensors)\n",
    "valid_imgs_preprocess = preprocess_input(valid_tensors)\n",
    "test_imgs_preprocess = preprocess_input(test_tensors)\n",
    "del train_tensors, valid_tensors, test_tensors\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10, 6, 1536)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "transfer_model = InceptionResNetV2(include_top=False)\n",
    "train_data = transfer_model.predict(train_imgs_preprocess)\n",
    "valid_data = transfer_model.predict(valid_imgs_preprocess)\n",
    "test_data = transfer_model.predict(test_imgs_preprocess)\n",
    "del train_imgs_preprocess,valid_imgs_preprocess,test_imgs_preprocess\n",
    "print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              3147776   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 3,153,923\n",
      "Trainable params: 3,153,923\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, Dropout, Flatten, Dense, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "BCD = Sequential()\n",
    "BCD.add(GlobalAveragePooling2D(input_shape=train_data.shape[1:]))\n",
    "BCD.add(Dropout(0.2))\n",
    "BCD.add(Dense(2048,activation='relu'))\n",
    "BCD.add(Dropout(0.3))\n",
    "BCD.add(Dense(3,activation='softmax'))\n",
    "BCD.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BCD.compile(loss='categorical_crossentropy',optimizer='adam',\n",
    "           metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 1.1430 - accuracy: 0.6396 ETA: 0s\n",
      "Epoch 00001: val_loss improved from inf to 1.40694, saving model to weights.hdf5\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 1.1426 - accuracy: 0.6395 - val_loss: 1.4069 - val_accuracy: 0.5200\n",
      "Epoch 2/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.7690 - accuracy: 0.6895\n",
      "Epoch 00002: val_loss improved from 1.40694 to 1.29560, saving model to weights.hdf5\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.7690 - accuracy: 0.6895 - val_loss: 1.2956 - val_accuracy: 0.5333\n",
      "Epoch 3/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.7336 - accuracy: 0.6817\n",
      "Epoch 00003: val_loss improved from 1.29560 to 0.80606, saving model to weights.hdf5\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.7333 - accuracy: 0.6820 - val_loss: 0.8061 - val_accuracy: 0.6667\n",
      "Epoch 4/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.7060 - accuracy: 0.6988\n",
      "Epoch 00004: val_loss improved from 0.80606 to 0.77050, saving model to weights.hdf5\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.7063 - accuracy: 0.6985 - val_loss: 0.7705 - val_accuracy: 0.6267\n",
      "Epoch 5/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.6994 - accuracy: 0.7047\n",
      "Epoch 00005: val_loss did not improve from 0.77050\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.7013 - accuracy: 0.7040 - val_loss: 0.9688 - val_accuracy: 0.5467\n",
      "Epoch 6/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.6666 - accuracy: 0.7220\n",
      "Epoch 00006: val_loss improved from 0.77050 to 0.75609, saving model to weights.hdf5\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.6666 - accuracy: 0.7220 - val_loss: 0.7561 - val_accuracy: 0.6533\n",
      "Epoch 7/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.6903 - accuracy: 0.7152\n",
      "Epoch 00007: val_loss did not improve from 0.75609\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.6899 - accuracy: 0.7155 - val_loss: 0.8248 - val_accuracy: 0.6400\n",
      "Epoch 8/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.6620 - accuracy: 0.7202\n",
      "Epoch 00008: val_loss did not improve from 0.75609\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.6619 - accuracy: 0.7205 - val_loss: 0.7864 - val_accuracy: 0.6667\n",
      "Epoch 9/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.6454 - accuracy: 0.7325 ETA: 0s - loss: 0.6455 - accuracy\n",
      "Epoch 00009: val_loss did not improve from 0.75609\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.6454 - accuracy: 0.7325 - val_loss: 0.8767 - val_accuracy: 0.6133\n",
      "Epoch 10/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.6389 - accuracy: 0.7250\n",
      "Epoch 00010: val_loss did not improve from 0.75609\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.6389 - accuracy: 0.7250 - val_loss: 0.7859 - val_accuracy: 0.6533\n",
      "Epoch 11/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.6199 - accuracy: 0.7369\n",
      "Epoch 00011: val_loss did not improve from 0.75609\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.6196 - accuracy: 0.7370 - val_loss: 0.9288 - val_accuracy: 0.5533\n",
      "Epoch 12/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.6154 - accuracy: 0.7402\n",
      "Epoch 00012: val_loss did not improve from 0.75609\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.6150 - accuracy: 0.7405 - val_loss: 0.8353 - val_accuracy: 0.6267\n",
      "Epoch 13/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.6080 - accuracy: 0.7432\n",
      "Epoch 00013: val_loss did not improve from 0.75609\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.6089 - accuracy: 0.7430 - val_loss: 1.0770 - val_accuracy: 0.5800\n",
      "Epoch 14/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.6099 - accuracy: 0.7399\n",
      "Epoch 00014: val_loss improved from 0.75609 to 0.74251, saving model to weights.hdf5\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.6101 - accuracy: 0.7395 - val_loss: 0.7425 - val_accuracy: 0.6667\n",
      "Epoch 15/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.6168 - accuracy: 0.7485\n",
      "Epoch 00015: val_loss did not improve from 0.74251\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.6168 - accuracy: 0.7485 - val_loss: 0.7612 - val_accuracy: 0.6733\n",
      "Epoch 16/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.5952 - accuracy: 0.7624\n",
      "Epoch 00016: val_loss did not improve from 0.74251\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.5952 - accuracy: 0.7625 - val_loss: 0.7841 - val_accuracy: 0.6733\n",
      "Epoch 17/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7559\n",
      "Epoch 00017: val_loss did not improve from 0.74251\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.6001 - accuracy: 0.7560 - val_loss: 0.8034 - val_accuracy: 0.6600\n",
      "Epoch 18/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.5658 - accuracy: 0.7629\n",
      "Epoch 00018: val_loss did not improve from 0.74251\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.5657 - accuracy: 0.7630 - val_loss: 0.7474 - val_accuracy: 0.6933\n",
      "Epoch 19/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5593 - accuracy: 0.7603\n",
      "Epoch 00019: val_loss improved from 0.74251 to 0.64359, saving model to weights.hdf5\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.5595 - accuracy: 0.7600 - val_loss: 0.6436 - val_accuracy: 0.7067\n",
      "Epoch 20/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5442 - accuracy: 0.7708\n",
      "Epoch 00020: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.5437 - accuracy: 0.7710 - val_loss: 0.7273 - val_accuracy: 0.7200\n",
      "Epoch 21/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.5672 - accuracy: 0.7565\n",
      "Epoch 00021: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.5672 - accuracy: 0.7565 - val_loss: 0.7340 - val_accuracy: 0.7067\n",
      "Epoch 22/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7668\n",
      "Epoch 00022: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.5500 - accuracy: 0.7665 - val_loss: 0.7616 - val_accuracy: 0.7000\n",
      "Epoch 23/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5635 - accuracy: 0.7558\n",
      "Epoch 00023: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.5638 - accuracy: 0.7555 - val_loss: 0.6893 - val_accuracy: 0.7533\n",
      "Epoch 24/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5429 - accuracy: 0.7563\n",
      "Epoch 00024: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.5437 - accuracy: 0.7560 - val_loss: 0.7231 - val_accuracy: 0.6733\n",
      "Epoch 25/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5435 - accuracy: 0.7608\n",
      "Epoch 00025: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.5430 - accuracy: 0.7610 - val_loss: 0.8374 - val_accuracy: 0.6800\n",
      "Epoch 26/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.5586 - accuracy: 0.7579\n",
      "Epoch 00026: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.5589 - accuracy: 0.7575 - val_loss: 0.8378 - val_accuracy: 0.6533\n",
      "Epoch 27/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.5364 - accuracy: 0.7685 ETA: 0s -\n",
      "Epoch 00027: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 39s 19ms/step - loss: 0.5364 - accuracy: 0.7685 - val_loss: 0.7397 - val_accuracy: 0.7467\n",
      "Epoch 28/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.5359 - accuracy: 0.7659\n",
      "Epoch 00028: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 40s 20ms/step - loss: 0.5358 - accuracy: 0.7660 - val_loss: 0.8250 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5460 - accuracy: 0.7548\n",
      "Epoch 00029: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.5455 - accuracy: 0.7550 - val_loss: 0.7129 - val_accuracy: 0.7467\n",
      "Epoch 30/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.5123 - accuracy: 0.7919\n",
      "Epoch 00030: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.5127 - accuracy: 0.7915 - val_loss: 0.6983 - val_accuracy: 0.7400\n",
      "Epoch 31/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5194 - accuracy: 0.7763\n",
      "Epoch 00031: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.5202 - accuracy: 0.7760 - val_loss: 0.8899 - val_accuracy: 0.6933\n",
      "Epoch 32/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5460 - accuracy: 0.7703\n",
      "Epoch 00032: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.5462 - accuracy: 0.7700 - val_loss: 0.7715 - val_accuracy: 0.6933\n",
      "Epoch 33/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.5533 - accuracy: 0.7574\n",
      "Epoch 00033: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.5537 - accuracy: 0.7570 - val_loss: 0.7620 - val_accuracy: 0.7000\n",
      "Epoch 34/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.5163 - accuracy: 0.7769 ETA: 0s - loss: 0\n",
      "Epoch 00034: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.5163 - accuracy: 0.7770 - val_loss: 0.7285 - val_accuracy: 0.7000\n",
      "Epoch 35/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.5236 - accuracy: 0.7779\n",
      "Epoch 00035: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.5235 - accuracy: 0.7780 - val_loss: 0.8574 - val_accuracy: 0.7067\n",
      "Epoch 36/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.5118 - accuracy: 0.7784\n",
      "Epoch 00036: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.5115 - accuracy: 0.7785 - val_loss: 0.8401 - val_accuracy: 0.7067\n",
      "Epoch 37/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.7795\n",
      "Epoch 00037: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.5067 - accuracy: 0.7795 - val_loss: 0.9472 - val_accuracy: 0.6867\n",
      "Epoch 38/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5064 - accuracy: 0.7813\n",
      "Epoch 00038: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.5063 - accuracy: 0.7815 - val_loss: 0.7285 - val_accuracy: 0.7000\n",
      "Epoch 39/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5012 - accuracy: 0.7803\n",
      "Epoch 00039: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 34s 17ms/step - loss: 0.5019 - accuracy: 0.7795 - val_loss: 0.7997 - val_accuracy: 0.7000\n",
      "Epoch 40/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5040 - accuracy: 0.7753\n",
      "Epoch 00040: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 35s 17ms/step - loss: 0.5042 - accuracy: 0.7750 - val_loss: 0.7878 - val_accuracy: 0.6800\n",
      "Epoch 41/60\n",
      "1997/2000 [============================>.] - ETA: 0s - loss: 0.5029 - accuracy: 0.7797\n",
      "Epoch 00041: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 34s 17ms/step - loss: 0.5024 - accuracy: 0.7800 - val_loss: 0.8231 - val_accuracy: 0.6800\n",
      "Epoch 42/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.7780\n",
      "Epoch 00042: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.5102 - accuracy: 0.7780 - val_loss: 1.0121 - val_accuracy: 0.6667\n",
      "Epoch 43/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.4938 - accuracy: 0.7974\n",
      "Epoch 00043: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 36s 18ms/step - loss: 0.4945 - accuracy: 0.7970 - val_loss: 0.7388 - val_accuracy: 0.7200\n",
      "Epoch 44/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.4768 - accuracy: 0.7990\n",
      "Epoch 00044: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.4768 - accuracy: 0.7990 - val_loss: 0.8184 - val_accuracy: 0.7067\n",
      "Epoch 45/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.4900 - accuracy: 0.7873\n",
      "Epoch 00045: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.4895 - accuracy: 0.7875 - val_loss: 0.6824 - val_accuracy: 0.7067\n",
      "Epoch 46/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.4979 - accuracy: 0.7803\n",
      "Epoch 00046: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.4976 - accuracy: 0.7805 - val_loss: 0.7746 - val_accuracy: 0.7067\n",
      "Epoch 47/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.4826 - accuracy: 0.7829\n",
      "Epoch 00047: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.4823 - accuracy: 0.7830 - val_loss: 0.8148 - val_accuracy: 0.7067\n",
      "Epoch 48/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.4791 - accuracy: 0.7913\n",
      "Epoch 00048: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 39s 19ms/step - loss: 0.4787 - accuracy: 0.7915 - val_loss: 0.7756 - val_accuracy: 0.7267\n",
      "Epoch 49/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.4701 - accuracy: 0.7923\n",
      "Epoch 00049: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.4697 - accuracy: 0.7925 - val_loss: 0.6942 - val_accuracy: 0.7267\n",
      "Epoch 50/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.4930 - accuracy: 0.7873\n",
      "Epoch 00050: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.4940 - accuracy: 0.7870 - val_loss: 0.8616 - val_accuracy: 0.6867\n",
      "Epoch 51/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.4618 - accuracy: 0.7985\n",
      "Epoch 00051: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.4618 - accuracy: 0.7985 - val_loss: 0.8199 - val_accuracy: 0.7133\n",
      "Epoch 52/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.4725 - accuracy: 0.8023\n",
      "Epoch 00052: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 39s 20ms/step - loss: 0.4724 - accuracy: 0.8025 - val_loss: 0.9300 - val_accuracy: 0.7067\n",
      "Epoch 53/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.5003 - accuracy: 0.7793\n",
      "Epoch 00053: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 39s 19ms/step - loss: 0.4999 - accuracy: 0.7795 - val_loss: 0.9741 - val_accuracy: 0.6400\n",
      "Epoch 54/60\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.4725 - accuracy: 0.7864 ETA: 0s - loss: 0.4\n",
      "Epoch 00054: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 38s 19ms/step - loss: 0.4723 - accuracy: 0.7865 - val_loss: 0.9290 - val_accuracy: 0.7067\n",
      "Epoch 55/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.4720 - accuracy: 0.7923\n",
      "Epoch 00055: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.4715 - accuracy: 0.7925 - val_loss: 0.8354 - val_accuracy: 0.6667\n",
      "Epoch 56/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.4596 - accuracy: 0.7925\n",
      "Epoch 00056: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.4596 - accuracy: 0.7925 - val_loss: 0.8501 - val_accuracy: 0.6933\n",
      "Epoch 57/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.7955\n",
      "Epoch 00057: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 40s 20ms/step - loss: 0.4574 - accuracy: 0.7955 - val_loss: 0.8655 - val_accuracy: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.4567 - accuracy: 0.8045\n",
      "Epoch 00058: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 18ms/step - loss: 0.4567 - accuracy: 0.8045 - val_loss: 0.9306 - val_accuracy: 0.7133\n",
      "Epoch 59/60\n",
      "1998/2000 [============================>.] - ETA: 0s - loss: 0.4482 - accuracy: 0.8018\n",
      "Epoch 00059: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.4479 - accuracy: 0.8020 - val_loss: 0.8911 - val_accuracy: 0.7400\n",
      "Epoch 60/60\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.7990\n",
      "Epoch 00060: val_loss did not improve from 0.64359\n",
      "2000/2000 [==============================] - 37s 19ms/step - loss: 0.4612 - accuracy: 0.7990 - val_loss: 0.9591 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c802e555c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import os \n",
    "checkpoint_filepath = 'weights.hdf5'\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    verbose=1,save_best_only=True)\n",
    "BCD.fit(train_data,train_targets,\n",
    "        validation_data=(valid_data,valid_targets),\n",
    "       epochs=60,batch_size=1,\n",
    "        callbacks=[checkpointer],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCD.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "my_predictions = [BCD.predict(np.expand_dims(feature, axis=0)) for feature in test_data]\n",
    "\n",
    "\n",
    "\n",
    "with open('my_transfer.csv', 'w', newline='') as csvfile:\n",
    "    result_writger = csv.writer(csvfile)\n",
    "    result_writger.writerow(['Id', 'task_1', 'task_2'])\n",
    "    for test_filepath, test_prediction in zip(test_files, my_predictions):\n",
    "        result_writger.writerow([test_filepath, test_prediction[0][0], test_prediction[0][2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
